{
  "config": {
    "step": {
      "user": {
        "data": {
          "api_key": "[%key:common::config_flow::data::api_key%]",
          "api_base": "[%key:common::config_flow::data::api_base%]"
        }
      }
    },
    "error": {
      "cannot_connect": "[%key:common::config_flow::error::cannot_connect%]",
      "invalid_auth": "[%key:common::config_flow::error::invalid_auth%]",
      "unknown": "[%key:common::config_flow::error::unknown%]"
    }
  },
  "options": {
    "step": {
      "init": {
        "data": {
          "prompt": "Instructions",
          "chat_model": "[%key:common::generic::model%]",
          "max_tokens": "Maximum tokens to return in response",
          "temperature": "Temperature",
          "top_p": "Top P",
          "llm_hass_api": "[%key:common::config_flow::data::llm_hass_api%]",
          "recommended": "Recommended model settings",
          "reasoning_effort": "Reasoning effort",
          "web_search": "Enable web search",
          "search_context_size": "Search context size",
          "user_location": "Include home location",
          "ssl_verify": "Verify SSL certificate",
          "log_payload_request": "Log request payloads",
          "log_payload_response": "Log response payloads",
          "log_system_message": "Log system message",
          "early_wait_enable": "Enable early wait",
          "vocabulary_enable": "Enable vocabulary normalization",
          "log_utterances": "Log user utterances",
          "mcp_enabled": "Enable MCP Server (Stateful Conversations)",
          "local_intent_enable": "Enable Local Intent Handling"
        },
        "data_description": {
          "prompt": "Instruct how the LLM should respond. This can be a template.",
          "reasoning_effort": "How many reasoning tokens the model should generate before creating a response to the prompt (for certain reasoning models)",
          "web_search": "Allow the model to search the web for the latest information before generating a response",
          "search_context_size": "High level guidance for the amount of context window space to use for the search",
          "user_location": "Refine search results based on geography",
          "ssl_verify": "Verify the SSL certificate of the Azure OpenAI endpoint. Disable only for testing with self-signed certificates.",
          "log_payload_request": "Log the full request payload sent to the Azure OpenAI API. This is useful for debugging but can be verbose.",
          "log_payload_response": "Log the full response payload received from the Azure OpenAI API. This is useful for debugging but can be verbose.",
          "log_system_message": "Log the generated system message that is sent to the model. This is useful for debugging the context provided to the LLM.",
          "early_wait_enable": "If the model takes too long to respond, the agent will ask if you want to wait. The response will be delivered via a notification.",
          "vocabulary_enable": "Normalize user input by replacing synonyms (e.g., 'turn off' and 'deactivate' are treated as the same). This improves local intent recognition.",
          "log_utterances": "Log original and normalized user utterances to a file. This is useful for debugging the text normalization process.",
          "mcp_enabled": "Enable the Master Control Program (MCP) server to maintain conversation context. This sends only entity state changes after the first message, reducing token usage.",
          "local_intent_enable": "Handle simple commands like turning lights on/off directly without calling the LLM. This is faster and cheaper."
        }
      }
    },
    "error": {
      "model_not_supported": "This model is not supported, please select a different model",
      "web_search_not_supported": "Web search is not supported by this model"
    }
  },
  "selector": {
    "reasoning_effort": {
      "options": {
        "low": "[%key:common::state::low%]",
        "medium": "[%key:common::state::medium%]",
        "high": "[%key:common::state::high%]"
      }
    },
    "search_context_size": {
      "options": {
        "low": "[%key:common::state::low%]",
        "medium": "[%key:common::state::medium%]",
        "high": "[%key:common::state::high%]"
      }
    },
    "stats_override_mode": {
      "options": {
        "default": "Default (Use Global Setting)",
        "enable": "Enable",
        "disable": "Disable"
      }
    }
  },
  "services": {
    "generate_image": {
      "name": "Generate image",
      "description": "Turns a prompt into an image",
      "fields": {
        "config_entry": {
          "name": "Config entry",
          "description": "The config entry to use for this action"
        },
        "prompt": {
          "name": "Prompt",
          "description": "The text to turn into an image",
          "example": "A photo of a dog"
        },
        "size": {
          "name": "Size",
          "description": "The size of the image to generate"
        },
        "quality": {
          "name": "Quality",
          "description": "The quality of the image that will be generated"
        },
        "style": {
          "name": "Style",
          "description": "The style of the generated image"
        }
      }
    },
    "generate_content": {
      "name": "Generate content",
      "description": "Sends a conversational query to ChatGPT including any attached image or PDF files",
      "fields": {
        "config_entry": {
          "name": "Config entry",
          "description": "The config entry to use for this action"
        },
        "prompt": {
          "name": "Prompt",
          "description": "The prompt to send"
        },
        "filenames": {
          "name": "Files",
          "description": "List of files to upload"
        }
      }
    }
  },
  "exceptions": {
    "invalid_config_entry": {
      "message": "Invalid config entry provided. Got {config_entry}"
    }
  }
}