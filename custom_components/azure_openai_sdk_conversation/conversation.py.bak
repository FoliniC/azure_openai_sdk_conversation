# File: /usr/share/hassio/homeassistant/custom_components/azure_openai_sdk_conversation/conversation.py
# -*- coding: utf-8 -*-
"""Conversation provider for Azure OpenAI with optional web-search context, synonym normalization,  
local intent execution (turn_on/turn_off) and safe speech fallback."""

from __future__ import annotations

import asyncio
import json
import logging
import os
import re
import time
from datetime import datetime, timezone
from typing import Any

import httpx

from homeassistant.components import conversation
from homeassistant.components.conversation import (
    AbstractConversationAgent,
    ConversationInput,
)
from homeassistant.config_entries import ConfigEntry
from homeassistant.const import CONF_API_KEY
from homeassistant.core import HomeAssistant
from homeassistant.helpers import llm
from homeassistant.helpers import intent as intent_helper
from homeassistant.helpers import (
    area_registry as ar,
    device_registry as dr,
    entity_registry as er,
)
from homeassistant.helpers.entity_platform import AddEntitiesCallback
from homeassistant.helpers.httpx_client import get_async_client
from homeassistant.helpers.template import Template as HATemplate
from homeassistant.helpers.typing import HomeAssistantType

from .const import (
    CONF_LOG_LEVEL,
    CONF_LOG_MAX_PAYLOAD_CHARS,
    CONF_LOG_MAX_SSE_LINES,
    CONF_LOG_PAYLOAD_REQUEST,
    CONF_LOG_PAYLOAD_RESPONSE,
    CONF_LOG_SYSTEM_MESSAGE,
    CONF_WEB_SEARCH,
    CONF_WEB_SEARCH_CONTEXT_SIZE,
    CONF_WEB_SEARCH_USER_LOCATION,
    DEFAULT_LOG_LEVEL,
    DEFAULT_LOG_MAX_PAYLOAD_CHARS,
    DEFAULT_LOG_MAX_SSE_LINES,
    LOG_LEVEL_ERROR,
    LOG_LEVEL_INFO,
    LOG_LEVEL_NONE,
    LOG_LEVEL_TRACE,
    RECOMMENDED_WEB_SEARCH_CONTEXT_SIZE,
    # early wait + vocabolario
    CONF_EARLY_WAIT_ENABLE,
    CONF_EARLY_WAIT_SECONDS,
    CONF_VOCABULARY_ENABLE,
    CONF_SYNONYMS_FILE,
    # utterances log
    CONF_LOG_UTTERANCES,
    CONF_UTTERANCES_LOG_PATH,
)
from .search import WebSearchClient

DOMAIN = "azure_openai_sdk_conversation"

# Opzioni legacy/extra per retrocompatibilità
CONF_ENABLE_SEARCH = "enable_web_search"
CONF_BING_KEY = "bing_api_key"
CONF_BING_ENDPOINT = "bing_endpoint"
CONF_BING_MAX = "bing_max_results"

# Opzioni extra
CONF_CONV_API_VERSION = "conversation_api_version"
CONF_FORCE_MODE = "force_responses_mode"

# Altre chiavi opzioni configurabili
CONF_ENDPOINT = "endpoint"
CONF_DEPLOYMENT = "deployment"

# Debug SSE
CONF_DEBUG_SSE = "debug_sse"
CONF_DEBUG_SSE_LINES = "debug_sse_lines"

# Early-fail legacy key (compat)
LEGACY_CONF_EARLY_TIMEOUT_SECONDS = "early_timeout_seconds"

# Intent locale per comandi semplici (accendi/spegni)
CONF_LOCAL_INTENT_ENABLE = "local_intent_enable"

# Sentinel opzione "usa stessa api_version globale"
_SENTINEL_SAME = "__same__"

_LOGGER = logging.getLogger(__name__)


class AzureOpenAIConversationAgent(AbstractConversationAgent):
    """Conversation agent che usa Azure OpenAI; può includere risultati web."""

    def __init__(self, hass: HomeAssistantType, conf: dict[str, Any]) -> None:
        super().__init__()
        self._hass = hass
        self._conf = conf
        self._http = get_async_client(hass)

        # Endpoint e parametri Azure
        self._endpoint: str = (conf.get(CONF_ENDPOINT) or conf.get("api_base", "")).rstrip("/")
        self._deployment: str = conf.get(CONF_DEPLOYMENT) or conf.get("chat_model", "")
        self._api_version: str = self._normalize_api_version(conf)
        self._timeout: int = self._coerce_int(conf.get("api_timeout"), 30)
        self._force_mode: str = conf.get(CONF_FORCE_MODE, "auto")  # auto|responses|chat

        # Early wait: abilita + secondi (con fallback legacy)
        self._early_wait_enabled: bool = self._coerce_bool(conf.get(CONF_EARLY_WAIT_ENABLE), True)
        early_secs = conf.get(CONF_EARLY_WAIT_SECONDS, conf.get(LEGACY_CONF_EARLY_TIMEOUT_SECONDS, 5))
        self._early_timeout: int = max(1, self._coerce_int(early_secs, 5))

        self._headers_json = {
            "api-key": conf[CONF_API_KEY],
            "Content-Type": "application/json",
        }
        self._headers_sse = {
            "api-key": conf[CONF_API_KEY],
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        # Impostazioni logging
        self._log_level: str = str(conf.get(CONF_LOG_LEVEL, DEFAULT_LOG_LEVEL)).strip().lower()
        self._log_payload_request: bool = bool(conf.get(CONF_LOG_PAYLOAD_REQUEST, False))
        self._log_payload_response: bool = bool(conf.get(CONF_LOG_PAYLOAD_RESPONSE, False))
        self._log_system_message: bool = bool(conf.get(CONF_LOG_SYSTEM_MESSAGE, False))
        self._log_max_payload_chars: int = self._coerce_int(
            conf.get(CONF_LOG_MAX_PAYLOAD_CHARS), DEFAULT_LOG_MAX_PAYLOAD_CHARS
        )
        self._log_max_sse_lines: int = self._coerce_int(
            conf.get(CONF_LOG_MAX_SSE_LINES), DEFAULT_LOG_MAX_SSE_LINES
        )

        # Debug SSE
        self._debug_sse: bool = self._coerce_bool(conf.get(CONF_DEBUG_SSE), False)
        self._debug_sse_lines: int = self._coerce_int(conf.get(CONF_DEBUG_SSE_LINES), 10)

        # Web Search opzionale: supporto sia a chiave legacy (enable_web_search) sia a nuova (web_search)
        enable_search = self._coerce_bool(conf.get(CONF_ENABLE_SEARCH) or conf.get(CONF_WEB_SEARCH), False)
        self._search: WebSearchClient | None = None
        if enable_search:
            self._search = WebSearchClient(
                api_key=str(conf.get(CONF_BING_KEY, "")),
                endpoint=conf.get(CONF_BING_ENDPOINT, WebSearchClient.BING_ENDPOINT_DEFAULT),
                max_results=self._coerce_int(conf.get(CONF_BING_MAX), 5),
            )

        # Vocabolario: abilita/disable + file sinonimi (fallback per chiavi legacy)
        self._vocabulary_enable: bool = self._coerce_bool(conf.get(CONF_VOCABULARY_ENABLE), True)
        syn_file = conf.get(CONF_SYNONYMS_FILE)
        if syn_file:
            syn_file = syn_file.strip()
            if not os.path.isabs(syn_file):
                syn_file = hass.config.path(syn_file)
        self._synonyms_file: str | None = syn_file
        self._synonyms_loaded: bool = False
        self._token_synonyms: dict[str, str] = {}  # "luci" -> "luce"
        self._compiled_regex_rules: list[tuple[re.Pattern[str], str]] = []  # (pattern, replace)

        # Intent locale
        self._local_intent_enable: bool = self._coerce_bool(conf.get(CONF_LOCAL_INTENT_ENABLE), True)
        self._stopwords_it: set[str] = {
            "il",
            "lo",
            "la",
            "i",
            "gli",
            "le",
            "l'",
            "del",
            "dello",
            "della",
            "dei",
            "degli",
            "delle",
            "di",
            "in",
            "sul",
            "sulla",
            "sui",
            "nelle",
            "nel",
            "nella",
            "alla",
            "al",
            "allo",
            "alle",
            "ai",
            "agli",
            "per",
            "da",
            "con",
            "su",
            "a",
        }

        # Log frasi passate al modello
        self._log_utterances: bool = self._coerce_bool(conf.get(CONF_LOG_UTTERANCES), True)
        # Percorso: default .storage; se l'utente indica una cartella, aggiungi filename
        default_utt_path = hass.config.path(".storage/azure_openai_conversation_utterances.log")
        user_path = conf.get(CONF_UTTERANCES_LOG_PATH)
        final_path: str
        if not user_path:
            final_path = default_utt_path
        else:
            p = str(user_path).strip()
            if not os.path.isabs(p):
                p = hass.config.path(p)
            # Se termina con separatore o non ha estensione .log, ma è una directory esistente,
            # o sembra una cartella, allora aggiungi un filename di default.
            if p.endswith(os.sep) or p.endswith("/") or p.endswith("\\"):
                p = os.path.join(p, "azure_openai_conversation_utterances.log")
            else:
                # Se p punta ad una directory esistente
                try:
                    if os.path.isdir(p):
                        p = os.path.join(p, "azure_openai_conversation_utterances.log")
                except Exception:
                    pass
            final_path = p
        self._utterances_log_path: str = final_path

    # -------------------- helpers: logging wrappers --------------------

    @property
    def _lvl(self) -> int:
        if self._log_level == LOG_LEVEL_TRACE:
            return 3
        if self._log_level == LOG_LEVEL_INFO:
            return 2
        if self._log_level == LOG_LEVEL_ERROR:
            return 1
        return 0  # none

    def _log_debug(self, msg: str, *args: Any) -> None:
        if self._lvl >= 3:
            _LOGGER.debug(msg, *args)

    def _log_info(self, msg: str, *args: Any) -> None:
        if self._lvl >= 2:
            _LOGGER.info(msg, *args)

    def _log_warn(self, msg: str, *args: Any) -> None:
        if self._lvl >= 1:
            _LOGGER.warning(msg, *args)

    def _log_error(self, msg: str, *args: Any) -> None:
        # Sempre logga gli errori, indipendentemente dal livello selezionato
        _LOGGER.error(msg, *args)

    def _should_log_payload_request(self) -> bool:
        return self._lvl >= 2 and self._log_payload_request

    def _should_log_payload_response(self) -> bool:
        return self._lvl >= 2 and self._log_payload_response

    def _should_log_system_message(self) -> bool:
        return self._lvl >= 2 and self._log_system_message

    @staticmethod
    def _safe_json(obj: Any, max_len: int = 12000) -> str:
        try:
            s = json.dumps(obj, ensure_ascii=False)
        except Exception:
            try:
                s = str(obj)
            except Exception:
                s = "<unserializable>"
        if len(s) > max_len:
            return f"{s[:max_len]}... (truncated)"
        return s

    # -------------------- helpers: coercion --------------------

    @staticmethod
    def _coerce_int(value: Any, default: int) -> int:
        try:
            if isinstance(value, bool):
                return int(value)
            if isinstance(value, (int, float)):
                return int(value)
            if isinstance(value, str):
                v = value.strip()
                if not v:
                    return default
                return int(float(v))
        except Exception:
            return default
        return default

    @staticmethod
    def _coerce_float(value: Any, default: float) -> float:
        try:
            if isinstance(value, bool):
                return float(int(value))
            if isinstance(value, (int, float)):
                return float(value)
            if isinstance(value, str):
                v = value.strip()
                if not v:
                    return default
                return float(v)
        except Exception:
            return default
        return default

    @staticmethod
    def _coerce_bool(value: Any, default: bool) -> bool:
        if isinstance(value, bool):
            return value
        if isinstance(value, (int, float)):
            return value != 0
        if isinstance(value, str):
            v = value.strip().lower()
            if v in ("1", "true", "on", "yes", "y", "si", "s", "sì"):
                return True
            if v in ("0", "false", "off", "no", "n"):
                return False
        return default

    # -------------------- version helpers --------------------

    @staticmethod
    def _normalize_api_version(conf: dict[str, Any]) -> str:
        """Ritorna la api-version effettiva da usare, normalizzando il sentinel."""
        conv_ver = conf.get(CONF_CONV_API_VERSION)
        if not conv_ver or str(conv_ver).strip() in (_SENTINEL_SAME, ""):
            base = conf.get("api_version")
            return base or "2025-03-01-preview"
        return str(conv_ver).strip()

    @staticmethod
    def _ver_date_tuple(ver: str) -> tuple[int, int, int]:
        core = (ver or "").split("-preview")[0]
        parts = core.split("-")
        try:
            return (int(parts[0]), int(parts[1]), int(parts[2]))
        except Exception:  # noqa: BLE001
            return (1900, 1, 1)

    def _ensure_min_version(self, ver: str, minimum: str) -> str:
        """Ritorna 'ver' se >= minimum altrimenti 'minimum'."""
        v = self._ver_date_tuple(ver)
        m = self._ver_date_tuple(minimum)
        return ver if v >= m else minimum

    # -------------------- token param helpers --------------------

    def _chat_token_param_initial(self) -> str:
        """Determina il parametro token iniziale per Chat, evitando un primo tentativo errato."""
        # 1) Preferisci quanto validato/salvato in config (se disponibile)
        tp = str(self._conf.get("token_param") or "").strip()
        if tp in ("max_tokens", "max_completion_tokens"):
            return tp
        # 2) Euristica per famiglie di modelli recenti che richiedono max_completion_tokens anche su 2025-01
        model = (self._deployment or "").lower()
        if model.startswith("gpt-5") or model.startswith("gpt-4.1") or model.startswith("gpt-4.2"):
            return "max_completion_tokens"
        # 3) Fallback alla regola per api-version
        y, m, d = self._ver_date_tuple(self._api_version)
        return "max_completion_tokens" if (y, m, d) >= (2025, 3, 1) else "max_tokens"

    # -------------------- entity collection --------------------

    def _collect_exposed_entities(self) -> list[dict[str, Any]]:
        """Costruisce una lista di entità esposte all'assistente: entity_id, name, state, area, aliases[]."""
        area_reg = ar.async_get(self._hass)
        ent_reg = er.async_get(self._hass)
        dev_reg = dr.async_get(self._hass)

        out: list[dict[str, Any]] = []
        for st in self._hass.states.async_all():
            entry = ent_reg.async_get(st.entity_id)

            # Filtra: includi solo entità esposte all'assistente (Conversation/Assist)
            if not self._is_entity_exposed(entry):
                continue

            area_name = ""
            area_id = None
            if entry:
                area_id = entry.area_id
                if not area_id and entry.device_id:
                    dev = dev_reg.async_get(entry.device_id)
                    if dev and dev.area_id:
                        area_id = dev.area_id
            if area_id:
                area = area_reg.async_get_area(area_id)
                if area and area.name:
                    area_name = area.name

            out.append(
                {
                    "entity_id": st.entity_id,
                    "name": st.name or st.entity_id,
                    "state": st.state,
                    "area": area_name,
                    "aliases": [],  # alias non disponibili qui
                }
            )
        return out

    def _is_entity_exposed(self, entry: er.RegistryEntry | None) -> bool:
        """Ritorna True se l'entità è esposta all'assistente (flag Conversation nell'Entity Registry)."""
        if entry is None:
            return False
        try:
            opts = entry.options or {}
            conv_opts = opts.get(conversation.DOMAIN) or opts.get("conversation") or {}
            val = conv_opts.get("should_expose", conv_opts.get("expose", None))
            return bool(val)
        except Exception:
            return False

    @staticmethod
    def _format_val(val: Any) -> str:
        if isinstance(val, (str, int, float, bool)) or val is None:
            return str(val)
        try:
            return json.dumps(val, ensure_ascii=False)
        except Exception:  # noqa: BLE001
            return str(val)

    async def _render_system_message(self, raw_sys_msg: str, azure_ctx: dict[str, Any]) -> str:
        """Renderizza il system_message; fallback: sostituzione regex per {{ azure.* }}."""
        sys_msg = raw_sys_msg
        # 1) Prova render Jinja con HATemplate (azure + exposed_entities)
        try:
            tmpl = HATemplate(raw_sys_msg, hass=self._hass)
            sys_msg = tmpl.async_render(
                {
                    "azure": azure_ctx,
                    "exposed_entities": self._collect_exposed_entities(),
                }
            )
        except Exception as err:  # noqa: BLE001
            self._log_debug("System message template render failed, will try regex fallback: %s", err)

        # 2) Fallback: sostituisci tutti i {{ azure.xxx }} rimasti
        pat = re.compile(r"{{\s*azure\.([a-zA-Z0-9_]+)\s*}}")
        if pat.search(sys_msg):

            def _sub(m: re.Match[str]) -> str:
                key = m.group(1)
                return self._format_val(azure_ctx.get(key, ""))

            sys_msg = pat.sub(_sub, sys_msg)
        return sys_msg

    @property
    def supported_languages(self) -> list[str]:
        return ["en", "it"]

    # -------------------- quick messages and pending handling --------------------

    @staticmethod
    def _quick_fail_message(language: str | None, seconds: int) -> str:
        lang = (language or "").lower()
        if lang.startswith("it"):
            return (
                f"Nessuna risposta entro {seconds}s. "
                "Per continuare ad attendere, rispondi con il numero di secondi da attendere (es. 15). "
                "Qualsiasi altro testo o nessuna risposta interromperà l'attesa. "
                "La richiesta prosegue in background; controlla i log per i dettagli."
            )
        return (
            f"No response within {seconds}s. "
            "To keep waiting, reply with the number of seconds to wait (e.g., 15). "
            "Any other text or no reply will stop waiting. "
            "The request continues in background; check the logs for details."
        )

    def _pending_map(self) -> dict[str, Any]:
        root = self._hass.data.setdefault(DOMAIN, {})
        return root.setdefault("pending", {})

    def _purge_expired_pending(self) -> None:
        now = time.monotonic()
        pending = self._pending_map()
        drop: list[str] = []
        for key, rec in list(pending.items()):
            exp = rec.get("expire", now - 1)
            if now >= exp:
                drop.append(key)
        for key in drop:
            rec = pending.pop(key, None)
            if rec and isinstance(rec.get("task"), asyncio.Task) and not rec["task"].done():
                self._log_warn("Cancelling expired background request for conversation_id=%s", key)
                rec["task"].cancel()

    @staticmethod
    def _parse_wait_seconds(text: str, minimum: int = 1, maximum: int = 600) -> int | None:
        """Ritorna i secondi da attendere se il testo è un numero valido, altrimenti None."""
        m = re.fullmatch(r"\s*(\d+)\s*", text or "")
        if not m:
            return None
        try:
            val = int(m.group(1))
        except Exception:
            return None
        if val < minimum:
            return None
        if val > maximum:
            val = maximum
        return val

    def _safe_speech(self, language: str | None, text: str | None) -> str:
        """Garantisce una stringa di speech non vuota per evitare crash frontend."""
        s = (text or "").strip()
        if s:
            return s
        lang = (language or "").lower()
        if lang.startswith("it"):
            return "Non ho ricevuto alcuna risposta testuale dal modello."
        return "No textual answer was received from the model."

    # -------------------- synonyms: load, normalize, log --------------------

    def _default_synonyms_spec(self) -> dict[str, Any]:
        """Spec di default se non è fornito un file esterno."""
        return {
            "token_synonyms": {
                # verbi (IT)
                "disattiva": "spegni",
                "spegni": "spegni",
                "accendi": "accendi",
                "attiva": "accendi",
                "accendere": "accendi",
                "spegnere": "spegni",
                "on": "accendi",
                "off": "spegni",
                # oggetti/stanze (IT)
                "luci": "luce",
                "lampada": "luce",
                "lampadario": "luce",
                "faretti": "luce",
                # collocazioni comuni
                "luce tavolo": "tavolo",
                "luci tavolo": "tavolo",
                "tavolo cucina": "tavolo",
                "luce cucina": "cucina",
                "luci cucina": "cucina",
            },
            "regex_rules": [
                {"pattern": r"\b(spegni|accendi)\s+(il|lo|la|i|gli|le|l')\s+", "replace": r"\1 "},
                {"pattern": r"\b(luce|luci)\s+(del|della|dello|dei|degli|delle)\s+(tavolo)\b", "replace": r"\3"},
                {"pattern": r"\b(luce|luci)\s+tavolo\b", "replace": "tavolo"},
                {"pattern": r"\btavolo\s+cucina\b", "replace": "tavolo"},
                {"pattern": r"\s{2,}", "replace": " "},
            ],
        }

    async def _ensure_synonyms_loaded(self) -> None:
        if self._synonyms_loaded:
            return
        spec = self._default_synonyms_spec()
        if self._synonyms_file and os.path.isfile(self._synonyms_file):
            try:
                def _read() -> dict[str, Any]:
                    with open(self._synonyms_file, "r", encoding="utf-8") as f:
                        return json.load(f)

                loaded = await self._hass.async_add_executor_job(_read)
                if isinstance(loaded, dict):
                    # merge shallow
                    token_syn = dict(spec.get("token_synonyms", {}))
                    token_syn.update(loaded.get("token_synonyms", {}) or {})
                    spec["token_synonyms"] = token_syn
                    regex_rules = list(spec.get("regex_rules", []))
                    extr = loaded.get("regex_rules", [])
                    if isinstance(extr, list):
                        regex_rules.extend(extr)
                    spec["regex_rules"] = regex_rules
                    self._log_info(
                        "Loaded synonyms from %s (tokens=%d, regex_rules=%d)",
                        self._synonyms_file,
                        len(token_syn),
                        len(spec["regex_rules"]),
                    )
            except Exception as err:  # noqa: BLE001
                self._log_error("Failed to load synonyms file (%s): %r", self._synonyms_file, err)

        # Costruisci mappa e regex compilate
        token_synonyms: dict[str, str] = {}
        for k, v in (spec.get("token_synonyms") or {}).items():
            if not isinstance(k, str) or not isinstance(v, str):
                continue
            token_synonyms[k.strip().lower()] = v.strip().lower()
        self._token_synonyms = token_synonyms

        compiled: list[tuple[re.Pattern[str], str]] = []
        for rr in spec.get("regex_rules") or []:
            pat = rr.get("pattern")
            repl = rr.get("replace", "")
            if not isinstance(pat, str) or not isinstance(repl, str):
                continue
            try:
                compiled.append((re.compile(pat, flags=re.IGNORECASE), repl))
            except re.error as rex:
                self._log_warn("Invalid regex in synonyms spec: %s (err=%s)", pat, rex)
        self._compiled_regex_rules = compiled

        self._synonyms_loaded = True

    def _normalize_text(self, text: str) -> str:
        """Applica regole di normalizzazione e sinonimi al testo utente."""
        if not text:
            return text
        if not self._vocabulary_enable:
            # Vocabolario disabilitato: non alterare significativamente il testo.
            return text.strip()
        s = text.strip()
        # Applica regex rules (case-insensitive già nel compile)
        for pat, repl in self._compiled_regex_rules:
            s = pat.sub(repl, s)

        # Sostituzioni full-phrase ordinate per lunghezza
        if self._token_synonyms:
            for src in sorted(self._token_synonyms.keys(), key=lambda x: len(x), reverse=True):
                dst = self._token_synonyms[src]
                try:
                    s = re.sub(rf"\b{re.escape(src)}\b", dst, s, flags=re.IGNORECASE)
                except re.error:
                    s = s.replace(src, dst)

        # Spazi finali
        s = re.sub(r"\s{2,}", " ", s).strip()
        # Lowercase uniforme (manteniamo accenti)
        s = s.lower()
        return s

    async def _append_utterance_log(
        self,
        conversation_id: str | None,
        mode: str,
        original: str,
        normalized: str,
    ) -> None:
        if not self._log_utterances or not self._utterances_log_path:
            return

        line = json.dumps(
            {
                "ts": datetime.now(timezone.utc).isoformat(),
                "conversation_id": conversation_id or "",
                "mode": mode,
                "original": original,
                "normalized": normalized,
            },
            ensure_ascii=False,
        )

        def _write() -> None:
            path = self._utterances_log_path
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "a", encoding="utf-8") as f:
                f.write(line + "\n")

        try:
            await self._hass.async_add_executor_job(_write)
        except Exception as err:  # noqa: BLE001
            # errore sempre visibile
            _LOGGER.error("Failed writing utterance log to %s: %r", self._utterances_log_path, err)

    # -------------------- local intent: parse, match, execute --------------------

    def _extract_onoff_intent(self, text: str) -> tuple[str, list[str]] | None:
        """Rileva un comando accendi/spegni e restituisce (azione, tokens)."""
        if not text:
            return None
        m = re.match(r"^\s*(spegni|accendi)\b(.*)$", text, flags=re.IGNORECASE)
        if not m:
            return None
        action = m.group(1).lower()
        rest = (m.group(2) or "").strip().lower()
        raw_tokens = re.findall(r"[a-zàèéìòóù]+", rest)
        tokens: list[str] = []
        for t in raw_tokens:
            if t in self._stopwords_it:
                continue
            if t == "luce" and any(x for x in raw_tokens if x != "luce"):
                continue
            tokens.append(t)
        uniq = []
        for t in tokens:
            if t not in uniq:
                uniq.append(t)
        tokens = uniq
        return (action, tokens)

    def _match_entities_for_tokens(self, tokens: list[str]) -> list[dict[str, Any]]:
        """Trova entità candidate in base ai tokens (preferisce light/switch) e calcola uno score."""
        if not tokens:
            return []
        candidates = self._collect_exposed_entities()
        out: list[tuple[float, dict[str, Any]]] = []

        tset = set(tokens)
        for ent in candidates:
            eid = ent["entity_id"]
            domain = eid.split(".", 1)[0]
            if domain not in ("light", "switch"):
                continue
            name = (ent.get("name") or "").lower()
            area = (ent.get("area") or "").lower()

            score = 0.0
            for tok in tset:
                if not tok:
                    continue
                if tok == area:
                    score += 4.0
                if tok and tok in name:
                    score += 3.0
                if tok and tok in eid.lower():
                    score += 1.5
            if "tavolo" in tset and ("table" in name or "desk" in name):
                score += 1.0

            if score > 0:
                out.append((score, ent))

        out.sort(key=lambda x: x[0], reverse=True)
        if not out:
            return []
        top_score = out[0][0]
        return [ent for sc, ent in out if sc >= (top_score * 0.9)]

    async def _execute_onoff(self, action: str, entity_ids: list[str]) -> list[tuple[str, bool, str]]:
        """Esegue homeassistant.turn_on/off sugli entity_ids, ritorna [(entity_id, ok, err_msg)]."""
        results: list[tuple[str, bool, str]] = []
        if not entity_ids:
            return results
        service = "turn_off" if action == "spegni" else "turn_on"
        data = {"entity_id": entity_ids}
        try:
            await self._hass.services.async_call("homeassistant", service, data, blocking=True)
            for eid in entity_ids:
                results.append((eid, True, ""))
            self._log_info("Local intent executed %s on %s", service, entity_ids)
        except Exception as err:  # noqa: BLE001
            msg = f"{type(err).__name__}: {err}"
            for eid in entity_ids:
                results.append((eid, False, msg))
            self._log_error("Local intent failed %s on %s: %s", service, entity_ids, msg)
        return results

    def _summarize_execution(self, language: str | None, action: str, exec_results: list[tuple[str, bool, str]]) -> str:
        """Crea un testo riassuntivo dell'azione eseguita."""
        lang_it = (language or "").lower().startswith("it")
        verb = "spento" if action == "spegni" else "acceso"
        if lang_it:
            if not exec_results:
                return "Non ho trovato alcuna entità corrispondente da controllare."
            lines = []
            for eid, ok, err in exec_results:
                st = self._hass.states.get(eid)
                fname = (st and (st.name or eid)) or eid
                if ok:
                    lines.append(f"- {fname}: {('off' if action == 'spegni' else 'on')}.")
                else:
                    lines.append(f"- {fname}: errore ({err}).")
            header = f"Sono {('in procinto di ' if any(ok for _, ok, _ in exec_results) else '')}{verb} "
            return f"{header}i dispositivi richiesti:\n" + "\n".join(lines)
        if not exec_results:
            return "I couldn't find any matching entities to control."
        lines = []
        for eid, ok, err in exec_results:
            st = self._hass.states.get(eid)
            fname = (st and (st.name or eid)) or eid
            if ok:
                lines.append(f"- {fname}: {('off' if action == 'spegni' else 'on')}.")
            else:
                lines.append(f"- {fname}: error ({err}).")
        header = f"I have {('started to ' if any(ok for _, ok, _ in exec_results) else '')}{('turn off' if action == 'spegni' else 'turn on')} "
        return f"{header}the requested devices:\n" + "\n".join(lines)

    # -------------------- core process --------------------

    async def async_process(self, user_input: ConversationInput):
        """Return the assistant response (non streaming per HA)."""
        self._purge_expired_pending()
        if self._vocabulary_enable:
            await self._ensure_synonyms_loaded()

        conv_id = user_input.conversation_id
        text_raw = (user_input.text or "").strip()
        text_lower = text_raw.lower()
        if conv_id:
            pending = self._pending_map().get(conv_id)
            if pending:
                wait_secs = self._parse_wait_seconds(text_lower)
                if wait_secs is not None:
                    task: asyncio.Task = pending.get("task")
                    mode = pending.get("mode")
                    self._log_info(
                        "User requested to wait up to %ss for background %s request (task=%s)",
                        wait_secs,
                        mode,
                        task,
                    )
                    try:
                        result_text = await asyncio.wait_for(asyncio.shield(task), timeout=wait_secs)
                    except asyncio.TimeoutError:
                        msg = (
                            f"Ancora nessuna risposta dopo {wait_secs}s. "
                            "Rispondi con un altro numero di secondi per continuare ad attendere; "
                            "qualunque altro testo interromperà l'attesa. "
                            "La richiesta prosegue in background."
                            if (user_input.language or "").lower().startswith("it")
                            else f"Still no response after {wait_secs}s. "
                            "Reply with another number of seconds to keep waiting; "
                            "any other text will stop waiting. "
                            "The request is still running in the background."
                        )
                        response = intent_helper.IntentResponse(language=getattr(user_input, "language", None))
                        response.async_set_speech(msg)
                        return conversation.ConversationResult(
                            response=response,
                            conversation_id=conv_id,
                        )
                    except Exception as err:  # noqa: BLE001
                        self._log_error(
                            "Background %s task failed (%s): %r",
                            mode,
                            type(err).__name__,
                            err,
                        )
                        result_text = ""
                    else:
                        self._pending_map().pop(conv_id, None)

                    response = intent_helper.IntentResponse(language=getattr(user_input, "language", None))
                    speak = self._safe_speech(getattr(user_input, "language", None), result_text)
                    if not (result_text or "").strip():
                        self._log_warn("Background task completed with empty text; using fallback speech")
                    response.async_set_speech(speak)
                    return conversation.ConversationResult(
                        response=response,
                        conversation_id=conv_id,
                    )
                else:
                    mode = pending.get("mode")
                    self._log_info(
                        "User chose to stop waiting for background %s request; task will continue in background (%s)",
                        mode,
                        pending.get("task"),
                    )
                    stop_msg = (
                        "Ok, interrompo l'attesa. La richiesta prosegue in background; controlla i log."
                        if (user_input.language or "").lower().startswith("it")
                        else "Okay, stopping the wait. The request keeps running in background; check the logs."
                    )
                    response = intent_helper.IntentResponse(language=getattr(user_input, "language", None))
                    response.async_set_speech(stop_msg)
                    return conversation.ConversationResult(
                        response=response,
                        conversation_id=conv_id,
                    )

        normalized_text = self._normalize_text(text_raw)

        # INTENT LOCALE
        if self._local_intent_enable:
            intent_parsed = self._extract_onoff_intent(normalized_text)
            if intent_parsed:
                action, tokens = intent_parsed
                matches = self._match_entities_for_tokens(tokens)
                exec_results: list[tuple[str, bool, str]] = []
                if matches:
                    entity_ids = [m["entity_id"] for m in matches]
                    exec_results = await self._execute_onoff(action, entity_ids)
                else:
                    self._log_info("Local intent recognized (%s) but no matching entities for tokens=%s", action, tokens)

                summary = self._summarize_execution(getattr(user_input, "language", None), action, exec_results)
                await self._append_utterance_log(conv_id, "local_intent", text_raw, normalized_text)
                response = intent_helper.IntentResponse(language=getattr(user_input, "language", None))
                response.async_set_speech(self._safe_speech(getattr(user_input, "language", None), summary))
                return conversation.ConversationResult(
                    response=response,
                    conversation_id=user_input.conversation_id,
                )

        # Selezione endpoint
        if self._force_mode == "responses":
            use_responses = True
        elif self._force_mode == "chat":
            use_responses = False
        else:
            use_responses = bool(self._deployment and self._deployment.lower().startswith("o"))

        def _responses_token_param_for_version(ver: str) -> str:
            y, m, d = self._ver_date_tuple(ver)
            return "max_output_tokens" if (y, m, d) >= (2025, 3, 1) else "max_completion_tokens"

        effective_version_for_mode = (
            self._ensure_min_version(self._api_version, "2025-03-01-preview")
            if use_responses
            else self._api_version
        )
        token_param = (
            _responses_token_param_for_version(effective_version_for_mode)
            if use_responses
            else self._chat_token_param_initial()
        )

        search_enabled = self._coerce_bool(
            self._conf.get(CONF_ENABLE_SEARCH) or self._conf.get(CONF_WEB_SEARCH, False), False
        )
        default_ctx_size = RECOMMENDED_WEB_SEARCH_CONTEXT_SIZE if search_enabled else 0
        web_ctx_size = self._coerce_int(self._conf.get(CONF_WEB_SEARCH_CONTEXT_SIZE), default_ctx_size)
        if web_ctx_size < 0:
            web_ctx_size = 0
        if search_enabled and web_ctx_size == 0:
            web_ctx_size = RECOMMENDED_WEB_SEARCH_CONTEXT_SIZE

        azure_ctx = {
            "endpoint": self._endpoint,
            "deployment": self._deployment,
            "model": self._deployment,
            "api_version": effective_version_for_mode,
            "mode": "responses" if use_responses else "chat",
            "token_param": token_param,
            "max_tokens": self._coerce_int(self._conf.get("max_tokens"), 1024),
            "temperature": self._coerce_float(self._conf.get("temperature"), 0.7),
            "api_timeout": self._timeout,
            "search_enabled": search_enabled,
            "web_search_context_size": web_ctx_size,
            "web_search_user_location": self._coerce_bool(
                self._conf.get(CONF_WEB_SEARCH_USER_LOCATION), False
            ),
            "debug_sse": self._debug_sse,
            "early_timeout_seconds": self._early_timeout,
        }

        raw_sys_msg = self._conf.get("system_message") or "You are Home Assistant’s AI helper."
        sys_msg = await self._render_system_message(raw_sys_msg, azure_ctx)

        unresolved_azure = bool(re.search(r"{{\s*azure\.", sys_msg))
        contains_azure = "azure." in raw_sys_msg
        if unresolved_azure or not contains_azure:
            identity_text = (
                "Identità: assistente per Home Assistant.\n"
                f"Endpoint: {azure_ctx['endpoint']}\n"
                f"Deployment/Model: {azure_ctx['deployment']}\n"
                f"API version: {azure_ctx['api_version']}\n"
                f"Mode: {azure_ctx['mode']}\n"
                f"Token param: {azure_ctx['token_param']}\n"
                f"Max tokens: {azure_ctx['max_tokens']}\n"
                f"Temperature: {azure_ctx['temperature']}\n"
                f"Timeout: {azure_ctx['api_timeout']}s\n"
                f"Web search: {azure_ctx['search_enabled']}"
            )
            sys_msg = f"{sys_msg}\n\n{identity_text}"

        if self._should_log_system_message():
            self._log_info("System prompt sent to model:\n%s", sys_msg)

        messages_chat: list[dict[str, str]] = [{"role": "system", "content": sys_msg}]
        if self._search and search_enabled:
            query = normalized_text or text_raw
            try:
                search_md = await self._search.search(query)
            except Exception as err:  # noqa: BLE001
                self._log_warn("Web search failed: %s", err)
                search_md = ""
            if search_md:
                messages_chat.append(
                    {"role": "system", "content": "Real-time web search results:\n\n" + search_md}
                )

        user_msg_for_model = normalized_text if self._vocabulary_enable else text_raw
        messages_chat.append({"role": "user", "content": user_msg_for_model})

        mode_label = "responses" if use_responses else "chat"
        await self._append_utterance_log(conv_id, mode_label, text_raw, user_msg_for_model)

        self._log_debug(
            "Starting conversation using %s API (deployment=%s, api-version configured=%s)",
            "Responses" if use_responses else "Chat",
            self._deployment,
            self._api_version,
        )

        text_out = ""
        try:
            first_chunk_event = asyncio.Event()
            if use_responses:
                if self._early_wait_enabled and self._early_timeout > 0:
                    task = self._hass.async_create_task(
                        self._responses_stream_run(
                            messages_chat=messages_chat,
                            sys_msg=sys_msg,
                            initial_version=effective_version_for_mode,
                            first_event=first_chunk_event,
                        )
                    )
                    try:
                        await asyncio.wait_for(first_chunk_event.wait(), timeout=self._early_timeout)
                        text_out = await task
                    except asyncio.TimeoutError:
                        self._log_error(
                            "No first text chunk within %ss (Responses). "
                            "Keeping the request running in background (task=%s).",
                            self._early_timeout,
                            task,
                        )
                        exp = time.monotonic() + self._timeout + 120
                        if conv_id:
                            self._pending_map()[conv_id] = {
                                "task": task,
                                "mode": "responses",
                                "expire": exp,
                            }
                            task.add_done_callback(lambda fut, cid=conv_id: self._pending_map().pop(cid, None))
                        text_out = self._quick_fail_message(getattr(user_input, "language", None), self._early_timeout)
                else:
                    text_out = await self._responses_stream_run(
                        messages_chat=messages_chat,
                        sys_msg=sys_msg,
                        initial_version=effective_version_for_mode,
                        first_event=None,
                    )
            else:
                if self._early_wait_enabled and self._early_timeout > 0:
                    task = self._hass.async_create_task(
                        self._chat_stream_run(
                            messages_chat=messages_chat,
                            first_event=first_chunk_event,
                        )
                    )
                    try:
                        await asyncio.wait_for(first_chunk_event.wait(), timeout=self._early_timeout)
                        text_out = await task
                    except asyncio.TimeoutError:
                        self._log_error(
                            "No first text chunk within %ss (Chat). "
                            "Keeping the request running in background (task=%s).",
                            self._early_timeout,
                            task,
                        )
                        exp = time.monotonic() + self._timeout + 120
                        if conv_id:
                            self._pending_map()[conv_id] = {
                                "task": task,
                                "mode": "chat",
                                "expire": exp,
                            }
                            task.add_done_callback(lambda fut, cid=conv_id: self._pending_map().pop(cid, None))
                        text_out = self._quick_fail_message(getattr(user_input, "language", None), self._early_timeout)
                else:
                    text_out = await self._chat_stream_run(
                        messages_chat=messages_chat,
                        first_event=None,
                    )
        except Exception as err:  # noqa: BLE001
            if isinstance(err, (asyncio.TimeoutError, httpx.ReadTimeout)):
                self._log_error(
                    "Azure conversation processing failed (%s): request timed out after ~%ss",
                    type(err).__name__,
                    self._timeout,
                )
            else:
                self._log_error(
                    "Azure conversation processing failed (%s): %r",
                    type(err).__name__,
                    err,
                )
        if self._should_log_payload_response() and text_out:
            self._log_info("Response text: %s", text_out)

        response = intent_helper.IntentResponse(language=getattr(user_input, "language", None))
        speak = self._safe_speech(getattr(user_input, "language", None), text_out)
        if not (text_out or "").strip():
            self._log_warn("Model returned empty text; sending fallback speech to avoid frontend crash")
        response.async_set_speech(speak)

        return conversation.ConversationResult(
            response=response,
            conversation_id=user_input.conversation_id,
        )

    # -------------------- Responses streaming runner --------------------

    async def _responses_stream_run(
        self,
        messages_chat: list[dict[str, str]],
        sys_msg: str,
        initial_version: str,
        first_event: asyncio.Event | None = None,
    ) -> str:
        """Esegue lo streaming Responses con fallback e restituisce il testo aggregato."""
        url = f"{self._endpoint}/openai/responses"

        def _to_input(msgs: list[dict[str, str]]) -> list[dict[str, Any]]:
            out_items: list[dict[str, Any]] = []
            for m in msgs:
                out_items.append(
                    {
                        "role": m["role"],
                        "content": [{"type": "input_text", "text": m["content"]}],
                    }
                )
            return out_items

        text_out = ""
        next_version = self._ensure_min_version(initial_version, "2025-03-01-preview")
        attempted: set[str] = set()
        res_token_param: str | None = None
        use_messages_format = False
        first_seen = False

        while True:
            if res_token_param is None:
                y, mn, d = self._ver_date_tuple(next_version)
                res_token_param = "max_output_tokens" if (y, mn, d) >= (2025, 3, 1) else "max_completion_tokens"

            fmt = "messages" if use_messages_format else "input"
            pair_key = f"{next_version}::{res_token_param}::fmt={fmt}"
            if pair_key in attempted:
                break
            attempted.add(pair_key)

            payload: dict[str, Any] = {
                "model": self._deployment,
                res_token_param: self._coerce_int(self._conf.get("max_tokens"), 1024),
                "temperature": self._coerce_float(self._conf.get("temperature"), 0.7),
                "stream": True,
                "modalities": ["text"],
                "text": {"format": "text"},
            }
            if use_messages_format:
                payload["messages"] = _to_input(messages_chat)
                payload["instructions"] = sys_msg
            else:
                payload["input"] = _to_input(messages_chat)

            if self._should_log_payload_request():
                self._log_info(
                    "Request payload (Responses %s): %s",
                    fmt,
                    self._safe_json(payload, self._log_max_payload_chars),
                )

            try:
                async with self._http.stream(
                    "POST",
                    url,
                    params={"api-version": next_version},
                    headers=self._headers_sse,
                    json=payload,
                    timeout=self._timeout,
                ) as resp:
                    if resp.status_code >= 400:
                        body = await resp.aread()
                        text_body = body.decode("utf-8", "ignore")
                        try:
                            err_json = json.loads(text_body or "{}")
                        except Exception:
                            err_json = {}
                        msg = err_json.get("error", {}).get("message") or text_body or f"HTTP {resp.status_code}"
                        self._log_error("Azure responses stream error: %s", msg)

                        if (
                            "Responses API is enabled only for api-version 2025-03-01-preview" in msg
                            and next_version != "2025-03-01-preview"
                        ):
                            self._log_debug("Retrying Responses with api-version=2025-03-01-preview")
                            next_version = "2025-03-01-preview"
                            res_token_param = None
                            continue

                        if "Unsupported parameter: 'max_completion_tokens'" in msg and res_token_param != "max_output_tokens":
                            self._log_debug("Retrying Responses switching token param to max_output_tokens")
                            res_token_param = "max_output_tokens"
                            continue
                        if "Unsupported parameter: 'max_output_tokens'" in msg and res_token_param != "max_completion_tokens":
                            self._log_debug("Retrying Responses switching token param to max_completion_tokens")
                            res_token_param = "max_completion_tokens"
                            continue

                        if not use_messages_format:
                            self._log_debug("Retrying Responses switching to messages+instructions format")
                            use_messages_format = True
                            continue

                        break

                    last_event: str | None = None
                    current_event: str | None = None
                    data_lines: list[str] = []
                    debug_samples: list[str] = []
                    collect_sse_samples = self._debug_sse or self._should_log_payload_response()
                    if collect_sse_samples:
                        debug_limit = self._debug_sse_lines if self._debug_sse else self._log_max_sse_lines
                    else:
                        debug_limit = 0

                    async for raw_line in resp.aiter_lines():
                        if raw_line is None:
                            continue
                        line = raw_line.rstrip("\n\r")

                        if not line:
                            if not data_lines:
                                continue
                            data_str = "\n".join(data_lines).strip()
                            data_lines = []
                            if debug_limit > 0 and len(debug_samples) < debug_limit:
                                debug_samples.append(
                                    f"event={current_event or last_event} data={data_str[:500]}"
                                )
                            if not data_str or data_str == "[DONE]":
                                break
                            try:
                                payload_obj = json.loads(data_str)
                            except Exception:
                                current_event = None
                                continue

                            event_name = (
                                payload_obj.get("type")
                                or payload_obj.get("event")
                                or current_event
                                or last_event
                            )

                            def _consume(node: Any) -> None:
                                nonlocal text_out, first_seen
                                if node is None:
                                    return
                                if isinstance(node, str):
                                    text_out += node
                                    if not first_seen:
                                        first_seen = True
                                        if first_event:
                                            first_event.set()
                                    return
                                if isinstance(node, list):
                                    for it in node:
                                        _consume(it)
                                    return
                                if isinstance(node, dict):
                                    txt = node.get("text")
                                    if isinstance(txt, str):
                                        text_out += txt
                                        if not first_seen:
                                            first_seen = True
                                            if first_event:
                                                first_event.set()
                                    _consume(node.get("content"))
                                    _consume(node.get("delta"))
                                    _consume(node.get("data"))
                                    _consume(node.get("output"))
                                    _consume(node.get("message"))

                            if event_name in (
                                "response.output_text.delta",
                                "output_text.delta",
                                "response.delta",
                                "delta",
                                "response.message.delta",
                                "message.delta",
                                "response.refusal.delta",
                                "refusal.delta",
                                "response.output_text",
                            ):
                                _consume(payload_obj.get("delta") or payload_obj)
                            elif event_name in ("response.error",):
                                self._log_error("Azure responses error event: %s", payload_obj)
                                break
                            elif event_name in (
                                "response.completed",
                                "message.completed",
                                "response.finish",
                                "response.output_text.done",
                            ):
                                pass

                            last_event = event_name or last_event
                            current_event = None
                            continue

                        if line.startswith(":"):
                            continue
                        if line.startswith("event:"):
                            current_event = line[6:].strip()
                            continue
                        if line.startswith("data:"):
                            data_lines.append(line[5:].lstrip())
                            continue

                    if data_lines:
                        data_str = "\n".join(data_lines).strip()
                        if debug_limit > 0 and len(debug_samples) < debug_limit:
                            debug_samples.append(
                                f"event={current_event or last_event} data={data_str[:500]}"
                            )
                        try:
                            payload_obj = json.loads(data_str)
                        except Exception:
                            payload_obj = None

                        if isinstance(payload_obj, dict):

                            def _consume_tail(node: Any) -> None:
                                nonlocal text_out, first_seen
                                if node is None:
                                    return
                                if isinstance(node, str):
                                    text_out += node
                                    if not first_seen:
                                        first_seen = True
                                        if first_event:
                                            first_event.set()
                                    return
                                if isinstance(node, list):
                                    for it in node:
                                        _consume_tail(it)
                                    return
                                if isinstance(node, dict):
                                    txt = node.get("text")
                                    if isinstance(txt, str):
                                        text_out += txt
                                        if not first_seen:
                                            first_seen = True
                                            if first_event:
                                                first_event.set()
                                    _consume_tail(node.get("output"))
                                    _consume_tail(node.get("content"))
                                    _consume_tail(node.get("message"))
                                    _consume_tail(node.get("data"))
                                    _consume_tail(node.get("delta"))

                            _consume_tail(payload_obj)

                    if debug_samples and collect_sse_samples:
                        self._log_info(
                            "Responses SSE sample (first %d messages):\n%s",
                            len(debug_samples),
                            "\n".join(debug_samples),
                        )

                    if not text_out and not use_messages_format:
                        self._log_debug(
                            "Responses stream produced no text, retrying with messages+instructions format"
                        )
                        use_messages_format = True
                        continue

                    break
            except (asyncio.TimeoutError, httpx.ReadTimeout) as err:
                self._log_error(
                    "Responses streaming timeout (%s): ~%ss inactivity",
                    type(err).__name__,
                    self._timeout,
                )
                break
            except Exception as err:  # noqa: BLE001
                self._log_error("Responses streaming failed (%s): %r", type(err).__name__, err)
                break

        if not text_out:
            self._log_debug(
                "Responses stream produced no text; trying non-stream Responses (format=%s)",
                "messages" if use_messages_format else "input",
            )
            text_out = await self._responses_non_stream(
                messages_chat, sys_msg, next_version, use_messages_format
            )
            if text_out and first_event and not first_seen:
                first_event.set()

        if not text_out and not (self._deployment or "").lower().startswith("o"):
            self._log_debug("Responses non-stream produced no text; falling back to Chat Completions")
            text_out = await self._chat_stream_run(messages_chat)
            if text_out and first_event and not first_seen:
                first_event.set()

        return text_out

    # -------------------- Chat streaming runner --------------------

    async def _chat_stream_run(
        self,
        messages_chat: list[dict[str, str]],
        first_event: asyncio.Event | None = None,
    ) -> str:
        """Esegue Chat Completions (stream) e restituisce il testo aggregato."""
        def _chat_token_param_for_version(ver: str) -> str:
            y, m, d = self._ver_date_tuple(ver)
            return "max_completion_tokens" if (y, m, d) >= (2025, 3, 1) else "max_tokens"

        text_out = ""
        first_seen = False
        url = f"{self._endpoint}/openai/deployments/{self._deployment}/chat/completions"
        next_version = self._api_version
        token_param = self._chat_token_param_initial()
        attempted: set[str] = set()

        while True:
            pair_key = f"{next_version}::{token_param}"
            if pair_key in attempted:
                break
            attempted.add(pair_key)

            payload: dict[str, Any] = {
                "messages": messages_chat,
                "temperature": self._coerce_float(self._conf.get("temperature"), 0.7),
                "stream": True,
                token_param: self._coerce_int(self._conf.get("max_tokens"), 1024),
            }

            if self._should_log_payload_request():
                self._log_info(
                    "Request payload (Chat Completions): %s",
                    self._safe_json(payload, self._log_max_payload_chars),
                )

            try:
                async with self._http.stream(
                    "POST",
                    url,
                    params={"api-version": next_version},
                    headers=self._headers_sse,
                    json=payload,
                    timeout=self._timeout,
                ) as resp:
                    if resp.status_code >= 400:
                        body = await resp.aread()
                        txt = body.decode("utf-8", "ignore")
                        try:
                            err_json = json.loads(txt or "{}")
                        except Exception:
                            err_json = {}
                        msg = err_json.get("error", {}).get("message") or txt or f"HTTP {resp.status_code}"
                        self._log_error("Azure chat stream error: %s", msg)

                        if "Unsupported parameter: 'max_tokens'" in msg and token_param != "max_completion_tokens":
                            self._log_debug("Retrying Chat switching token param to max_completion_tokens")
                            token_param = "max_completion_tokens"
                            continue
                        if "Unsupported parameter: 'max_completion_tokens'" in msg and token_param != "max_tokens":
                            self._log_debug("Retrying Chat switching token param to max_tokens")
                            token_param = "max_tokens"
                            continue

                        if "api-version 2025-03-01-preview" in msg and next_version != "2025-03-01-preview":
                            self._log_debug("Retrying Chat with api-version=2025-03-01-preview")
                            next_version = "2025-03-01-preview"
                            token_param = _chat_token_param_for_version(next_version)
                            continue

                        break

                    sse_samples: list[str] = []
                    collect_samples = self._should_log_payload_response() or self._debug_sse
                    if collect_samples:
                        debug_limit = self._debug_sse_lines if self._debug_sse else self._log_max_sse_lines
                    else:
                        debug_limit = 0

                    async for raw_line in resp.aiter_lines():
                        if not raw_line:
                            continue
                        line = raw_line.strip()
                        if not line or line.startswith(":"):
                            continue
                        if not line.startswith("data:"):
                            continue

                        data_str = line[5:].lstrip()
                        if (
                            collect_samples
                            and debug_limit > 0
                            and len(sse_samples) < debug_limit
                            and data_str
                            and data_str != "[DONE]"
                        ):
                            sse_samples.append(data_str[:500])
                        if not data_str or data_str == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data_str)
                        except Exception:
                            continue

                        try:
                            choices = chunk.get("choices") or []
                            if not choices:
                                continue
                            delta = choices[0].get("delta") or {}
                            content = delta.get("content")
                            if content:
                                text_out += str(content)
                                if not first_seen:
                                    first_seen = True
                                    if first_event:
                                        first_event.set()
                        except Exception:
                            continue

                    if sse_samples and collect_samples:
                        self._log_info(
                            "Chat Completions SSE sample (first %d lines):\n%s",
                            len(sse_samples),
                            "\n".join(sse_samples),
                        )
            except (asyncio.TimeoutError, httpx.ReadTimeout) as err:
                self._log_error(
                    "Chat streaming timeout (%s): ~%ss inactivity",
                    type(err).__name__,
                    self._timeout,
                )
                break
            except Exception as err:  # noqa: BLE001
                self._log_error("Chat streaming failed (%s): %r", type(err).__name__, err)
                break

            break
        return text_out

    # -------------------- Responses non-stream fallback --------------------

    async def _responses_non_stream(
        self,
        messages_chat: list[dict[str, str]],
        sys_msg: str,
        api_version: str,
        use_messages_format: bool,
    ) -> str:
        """Esegue una chiamata Responses non-stream e restituisce il testo aggregato."""
        url = f"{self._endpoint}/openai/responses"

        def _to_input(msgs: list[dict[str, str]]) -> list[dict[str, Any]]:
            out: list[dict[str, Any]] = []
            for m in msgs:
                out.append(
                    {
                        "role": m["role"],
                        "content": [{"type": "input_text", "text": m["content"]}],
                    }
                )
            return out

        def _responses_token_param_for_version(ver: str) -> str:
            y, m, d = self._ver_date_tuple(ver)
            return "max_output_tokens" if (y, m, d) >= (2025, 3, 1) else "max_completion_tokens"

        token_param = _responses_token_param_for_version(api_version)
        payload: dict[str, Any] = {
            "model": self._deployment,
            token_param: self._coerce_int(self._conf.get("max_tokens"), 1024),
            "temperature": self._coerce_float(self._conf.get("temperature"), 0.7),
            "modalities": ["text"],
            "text": {"format": "text"},
        }
        if use_messages_format:
            payload["messages"] = _to_input(messages_chat)
            payload["instructions"] = sys_msg
        else:
            payload["input"] = _to_input(messages_chat)

        if self._should_log_payload_request():
            self._log_info(
                "Request payload (Responses non-stream %s): %s",
                "messages" if use_messages_format else "input",
                self._safe_json(payload, self._log_max_payload_chars),
            )

        try:
            resp = await self._http.post(
                url,
                params={"api-version": api_version},
                headers=self._headers_json,
                json=payload,
                timeout=self._timeout,
            )
        except Exception as err:  # noqa: BLE001
            self._log_error("Azure responses (non-stream) request failed (%s): %r", type(err).__name__, err)
            return ""

        if resp.status_code >= 400:
            text_body = await resp.aread()
            try:
                err_json = json.loads(text_body.decode("utf-8", "ignore") or "{}")
            except Exception:
                err_json = {}
            msg = (
                err_json.get("error", {}).get("message")
                or text_body.decode("utf-8", "ignore")
                or f"HTTP {resp.status_code}"
            )
            self._log_error("Azure responses (non-stream) error: %s", msg)
            return ""

        try:
            obj = resp.json()
        except Exception:
            try:
                raw = await resp.aread()
                obj = json.loads(raw.decode("utf-8", "ignore"))
            except Exception:
                obj = None

        if self._should_log_payload_response() and obj is not None:
            self._log_info(
                "Response payload (Responses non-stream): %s",
                self._safe_json(obj, self._log_max_payload_chars),
            )

        if not isinstance(obj, dict):
            return ""
        out_parts: list[str] = []

        def _acc(node: Any) -> None:
            if node is None:
                return
            if isinstance(node, str):
                out_parts.append(node)
                return
            if isinstance(node, list):
                for it in node:
                    _acc(it)
                return
            if isinstance(node, dict):
                txt = node.get("text")
                if isinstance(txt, str):
                    out_parts.append(txt)
                _acc(node.get("output"))
                _acc(node.get("content"))
                _acc(node.get("message"))
                _acc(node.get("data"))
                _acc(node.get("choices"))

        _acc(obj)
        text = "".join(out_parts).strip()
        self._log_debug("Responses non-stream extracted %d chars", len(text))
        return text

    async def async_close(self) -> None:
        """Clean up network clients."""
        if self._search:
            await self._search.close()


async def async_setup_entry(
    hass: HomeAssistant,
    config_entry: ConfigEntry,
    async_add_entities: AddEntitiesCallback,
) -> None:
    """Set up the conversation platform for the Azure OpenAI integration."""
    conf: dict[str, Any] = {
        CONF_API_KEY: config_entry.data[CONF_API_KEY],
        "api_base": config_entry.data.get("api_base", ""),
        "chat_model": config_entry.data.get("chat_model", ""),
        "api_version": config_entry.data.get("api_version"),
        **config_entry.options,
    }

    sys_prompt = (
        conf.get("system_message")
        or conf.get("system_prompt")
        or conf.get("prompt")
        or llm.DEFAULT_INSTRUCTIONS_PROMPT
    )
    conf["system_message"] = sys_prompt

    agent = AzureOpenAIConversationAgent(hass, conf=conf)

    try:
        conversation.async_set_agent(hass, config_entry, agent)
    except TypeError:
        try:
            conversation.async_set_agent(hass, agent)
        except TypeError:
            conversation.async_register_agent(
                hass,
                agent_id=f"{DOMAIN}.{config_entry.entry_id}",
                agent=agent,
            )
