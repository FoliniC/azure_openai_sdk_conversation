{
  "config": {
    "abort": {
      "already_configured": "Already configured. Only a single configuration possible."
    },
    "error": {
      "cannot_connect": "Failed to connect",
      "invalid_auth": "Invalid authentication",
      "unknown": "Unexpected error",
      "invalid_deployment": "Deployment not found (404). Check API Base and Model name.",
      "value_out_of_range": "Value out of range",
      "invalid_number": "Invalid number"
    },
    "step": {
      "user": {
        "description": "Get the API Base URL and version from the target URI under EndPoint in Azure AI Foundry or in the Azure portal.",
        "data": {
          "api_key": "API Key",
          "api_base": "API Base URL",
          "chat_model": "Model",
          "api_version": "API Version"
        },
        "data_description": {
          "api_key": "",
          "api_base": "Example base URL: https://xyz.cognitiveservices.azure.com"
        }
      },
      "params": {
        "data": {
            "api_timeout": "API Timeout",
            "max_tokens": "Maximum tokens to return in response",
            "temperature": "Temperature",
            "top_p": "Top P",
            "log_level": "Log Level",
            "log_payload_request": "Log request payloads",
            "log_payload_response": "Log response payloads",
            "log_system_message": "Log system message",
            "log_max_payload_chars": "Max Payload Characters to Log",
            "log_max_sse_lines": "Max SSE Lines to Log",
            "early_wait_enable": "Enable early wait",
            "early_wait_seconds": "Early Wait Seconds",
            "vocabulary_enable": "Enable vocabulary normalization",
            "synonyms_file": "Synonyms File Path",
            "log_utterances": "Log user utterances",
            "utterances_log_path": "Utterances Log Path",
            "local_intent_enable": "Enable Local Intent Handling",
            "stats_enable": "Enable Statistics",
            "stats_component_log_path": "Component Stats Log Path",
            "stats_llm_log_path": "LLM Stats Log Path",
            "mcp_enabled": "Enable MCP Server (Stateful Conversations)",
            "mcp_ttl_seconds": "MCP Context TTL (Seconds)",
            "tools_enable": "Enable Tool Calling",
            "tools_whitelist": "Tools Whitelist",
            "tools_max_iterations": "Max Tool Iterations",
            "tools_max_calls_per_minute": "Max Tool Calls Per Minute",
            "tools_parallel_execution": "Enable Parallel Tool Execution",
            "sliding_window_enable": "Enable Sliding Window",
            "sliding_window_max_tokens": "Sliding Window Max Tokens",
            "sliding_window_preserve_system": "Preserve System Prompt in Window"
        }
      }
    }
  },
  "options": {
    "step": {
      "init": {
        "description": "Current Model: {model}\nAPI Base: {api_base}",
        "data": {
          "prompt": "Instructions",
          "chat_model": "Model",
          "max_tokens": "Maximum tokens to return in response",
          "temperature": "Temperature",
          "top_p": "Top P",
          "llm_hass_api": "Do you want to use the LLM HASS API?",
          "recommended": "Recommended model settings",
          "reasoning_effort": "Reasoning effort",
          "web_search": "Enable web search",
          "search_context_size": "Search context size",
          "user_location": "Include home location",
          "ssl_verify": "Verify SSL certificate",
          "exposed_entities_limit": "Exposed Entities Limit",
          "api_timeout": "API Timeout (seconds)",
          "api_version": "API Version",
          "log_level": "Log Level",
          "log_payload_request": "Log request payloads",
          "log_payload_response": "Log response payloads",
          "log_system_message": "Log system message",
          "log_max_payload_chars": "Max Payload Characters to Log",
          "log_max_sse_lines": "Max SSE Lines to Log",
          "payload_log_path": "Payload Log Path",
          "early_wait_enable": "Enable early wait",
          "early_wait_seconds": "Early Wait Seconds",
          "vocabulary_enable": "Enable vocabulary normalization",
          "synonyms_file": "Synonyms File Path",
          "log_utterances": "Log user utterances",
          "utterances_log_path": "Utterances Log Path",
          "local_intent_enable": "Enable Local Intent Handling",
          "stats_override_mode": "Statistics Override Mode",
          "stats_aggregated_file": "Aggregated Stats File Path",
          "stats_aggregation_interval": "Stats Aggregation Interval (Minutes)",
          "mcp_enabled": "Enable MCP Server (Stateful Conversations)",
          "mcp_ttl_seconds": "MCP Context TTL (Seconds)",
          "sliding_window_enable": "Enable Sliding Window",
          "sliding_window_max_tokens": "Sliding Window Max Tokens",
          "sliding_window_preserve_system": "Preserve System Prompt in Window",
          "sliding_window_token_buffer": "Sliding Window Token Buffer",
          "tools_enable": "Enable Tool Calling",
          "tools_whitelist": "Tools Whitelist",
          "tools_max_iterations": "Max Tool Iterations",
          "tools_max_calls_per_minute": "Max Tool Calls Per Minute",
          "tools_parallel_execution": "Enable Parallel Tool Execution"
        },
        "data_description": {
          "prompt": "Instruct how the LLM should respond. This can be a template.",
          "reasoning_effort": "How many reasoning tokens the model should generate before creating a response to the prompt (for certain reasoning models)",
          "web_search": "Allow the model to search the web for the latest information before generating a response",
          "search_context_size": "High level guidance for the amount of context window space to use for the search",
          "user_location": "Refine search results based on geography",
          "ssl_verify": "Verify the SSL certificate of the Azure OpenAI endpoint. Disable only for testing with self-signed certificates.",
          "log_payload_request": "Log the full request payload sent to the Azure OpenAI API. This is useful for debugging but can be verbose.",
          "log_payload_response": "Log the full response payload received from the Azure OpenAI API. This is useful for debugging but can be verbose.",
          "log_system_message": "Log the generated system message that is sent to the model. This is useful for debugging the context provided to the LLM.",
          "early_wait_enable": "If the model takes too long to respond, the agent will ask if you want to wait. The response will be delivered via a notification.",
          "vocabulary_enable": "Normalize user input by replacing synonyms (e.g., 'turn off' and 'deactivate' are treated as the same). This improves local intent recognition.",
          "log_utterances": "Log original and normalized user utterances to a file. This is useful for debugging the text normalization process.",
          "mcp_enabled": "Enable the Master Control Program (MCP) server to maintain conversation context. This sends only entity state changes after the first message, reducing token usage.",
          "local_intent_enable": "Handle simple commands like turning lights on/off directly without calling the LLM. This is faster and cheaper.",
          "exposed_entities_limit": "Maximum number of entities to expose to the model. Increasing this may increase latency and token usage.",
          "api_timeout": "Timeout for API calls in seconds.",
          "tools_enable": "Allow the model to call Home Assistant tools/services.",
          "tools_whitelist": "Comma-separated list of domains or services to allow (e.g., 'light,switch' or 'light.turn_on').",
          "sliding_window_enable": "Maintain a rolling window of conversation history within token limits.",
          "sliding_window_max_tokens": "Maximum tokens to use for history before truncating.",
          "sliding_window_preserve_system": "Always keep the system prompt when truncating history.",
          "sliding_window_token_buffer": "Buffer to reserve to avoid hitting exact token limits."
        }
      }
    },
    "error": {
      "model_not_supported": "This model is not supported, please select a different model",
      "web_search_not_supported": "Web search is not supported by this model"
    }
  },
  "selector": {
    "reasoning_effort": {
      "options": {
        "low": "Low",
        "medium": "Medium",
        "high": "High"
      }
    },
    "search_context_size": {
      "options": {
        "low": "Low",
        "medium": "Medium",
        "high": "High"
      }
    },
    "stats_override_mode": {
      "options": {
        "default": "Default (Use Global Setting)",
        "enable": "Enable",
        "disable": "Disable"
      }
    }
  },
  "services": {
    "generate_image": {
      "name": "Generate image",
      "description": "Turns a prompt into an image",
      "fields": {
        "config_entry": {
          "name": "Config entry",
          "description": "The config entry to use for this action"
        },
        "prompt": {
          "name": "Prompt",
          "description": "The text to turn into an image",
          "example": "A photo of a dog"
        },
        "size": {
          "name": "Size",
          "description": "The size of the image to generate"
        },
        "quality": {
          "name": "Quality",
          "description": "The quality of the image that will be generated"
        },
        "style": {
          "name": "Style",
          "description": "The style of the generated image"
        }
      }
    },
    "generate_content": {
      "name": "Generate content",
      "description": "Sends a conversational query to ChatGPT including any attached image or PDF files",
      "fields": {
        "config_entry": {
          "name": "Config entry",
          "description": "The config entry to use for this action"
        },
        "prompt": {
          "name": "Prompt",
          "description": "The prompt to send"
        },
        "filenames": {
          "name": "Files",
          "description": "List of files to upload"
        }
      }
    }
  },
  "exceptions": {
    "invalid_config_entry": {
      "message": "Invalid config entry provided. Got {config_entry}"
    }
  }
}