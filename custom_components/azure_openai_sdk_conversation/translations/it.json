{
  "config": {
    "abort": {
      "already_configured": "Già configurato. È possibile una sola configurazione."
    },
    "error": {
      "cannot_connect": "Impossibile connettersi",
      "invalid_auth": "Autenticazione non valida",
      "unknown": "Errore imprevisto"
    },
    "step": {
      "user": {
        "description": "Ottieni l'URL di base dell'API e la versione dall'URI di destinazione in EndPoint in Azure AI Foundry o nel portale di Azure.",
        "data": {
          "api_key": "Chiave API",
          "api_base": "URL di base dell'API"
        },
        "data_description": {
          "api_key": "",
          "api_base": "URL di base di esempio: https://xyz.cognitiveservices.azure.com"
        }
      }
    }
  },
  "options": {
    "step": {
      "init": {
        "data": {
          "prompt": "Istruzioni",
          "chat_model": "Modello",
          "max_tokens": "Numero massimo di token da restituire nella risposta",
          "temperature": "Temperatura",
          "top_p": "Top P",
          "llm_hass_api": "Vuoi usare l'API LLM HASS?",
          "recommended": "Impostazioni del modello consigliate",
          "reasoning_effort": "Sforzo di ragionamento",
          "web_search": "Abilita la ricerca web",
          "search_context_size": "Dimensione del contesto di ricerca",
          "user_location": "Includi la posizione della casa",
          "ssl_verify": "Verifica certificato SSL",
          "log_payload_request": "Registra payload delle richieste",
          "log_payload_response": "Registra payload delle risposte",
          "log_system_message": "Registra messaggio di sistema",
          "early_wait_enable": "Abilita attesa anticipata",
          "vocabulary_enable": "Abilita normalizzazione vocabolario",
          "log_utterances": "Registra espressioni utente",
          "mcp_enabled": "Abilita Server MCP (Conversazioni Stateful)",
          "local_intent_enable": "Abilita Gestione Intenti Locali"
        },
        "data_description": {
          "prompt": "Indica come deve rispondere il LLM. Questo può essere un modello.",
          "reasoning_effort": "Quanti token di ragionamento il modello dovrebbe generare prima di creare una risposta al prompt (per alcuni modelli di ragionamento)",
          "web_search": "Consenti al modello di cercare nel web le informazioni più recenti prima di generare una risposta",
          "search_context_size": "Guida di alto livello per la quantità di spazio della finestra di contesto da utilizzare per la ricerca",
          "user_location": "Affina i risultati della ricerca in base alla geografia",
          "ssl_verify": "Verifica il certificato SSL dell'endpoint di Azure OpenAI. Disabilita solo per test con certificati autofirmati.",
          "log_payload_request": "Registra il payload completo della richiesta inviata all'API di Azure OpenAI. Utile per il debug ma può essere verboso.",
          "log_payload_response": "Registra il payload completo della risposta ricevuta dall'API di Azure OpenAI. Utile per il debug ma può essere verboso.",
          "log_system_message": "Registra il messaggio di sistema generato che viene inviato al modello. Utile per il debug del contesto fornito al LLM.",
          "early_wait_enable": "Se il modello impiega troppo tempo a rispondere, l'agente chiederà se si desidera attendere. La risposta verrà recapitata tramite una notifica.",
          "vocabulary_enable": "Normalizza l'input dell'utente sostituendo i sinonimi (es. 'spegni' e 'disattiva' sono trattati allo stesso modo). Migliora il riconoscimento degli intenti locali.",
          "log_utterances": "Registra le espressioni utente originali e normalizzate in un file. Utile per il debug del processo di normalizzazione del testo.",
          "mcp_enabled": "Abilita il server Master Control Program (MCP) per mantenere il contesto della conversazione. Invia solo le modifiche di stato delle entità dopo il primo messaggio, riducendo l'uso di token.",
          "local_intent_enable": "Gestisce comandi semplici come accendere/spegnere le luci direttamente senza chiamare il LLM. È più veloce ed economico."

        }
      }
    },
    "error": {
      "model_not_supported": "Questo modello non è supportato, seleziona un modello diverso",
      "web_search_not_supported": "La ricerca web non è supportata da questo modello"
    }
  },
  "selector": {
    "reasoning_effort": {
      "options": {
        "low": "Basso",
        "medium": "Medio",
        "high": "Alto"
      }
    },
    "search_context_size": {
      "options": {
        "low": "Basso",
        "medium": "Medio",
        "high": "Alto"
      }
    }
  },
  "services": {
    "generate_image": {
      "name": "Genera immagine",
      "description": "Trasforma un prompt in un'immagine",
      "fields": {
        "config_entry": {
          "name": "Voce di configurazione",
          "description": "La voce di configurazione da utilizzare per questa azione"
        },
        "prompt": {
          "name": "Prompt",
          "description": "Il testo da trasformare in un'immagine",
          "example": "Una foto di un cane"
        },
        "size": {
          "name": "Dimensione",
          "description": "La dimensione dell'immagine da generare"
        },
        "quality": {
          "name": "Qualità",
          "description": "La qualità dell'immagine che verrà generata"
        },
        "style": {
          "name": "Stile",
          "description": "Lo stile dell'immagine generata"
        }
      }
    },
    "generate_content": {
      "name": "Genera contenuto",
      "description": "Invia una query conversazionale a ChatGPT includendo eventuali file di immagine o PDF allegati",
      "fields": {
        "config_entry": {
          "name": "Voce di configurazione",
          "description": "La voce di configurazione da utilizzare per questa azione"
        },
        "prompt": {
          "name": "Prompt",
          "description": "Il prompt da inviare"
        },
        "filenames": {
          "name": "File",
          "description": "Elenco dei file da caricare"
        }
      }
    }
  },
  "exceptions": {
    "invalid_config_entry": {
      "message": "Voce di configurazione non valida fornita. Ricevuto {config_entry}"
    }
  }
}